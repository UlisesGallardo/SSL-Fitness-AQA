{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generate_triplets:\n",
    "    def __init__(self, path_json, path_videos, path_bar_trajectory, save_path):\n",
    "        self.path_json = path_json\n",
    "        self.path_videos = path_videos\n",
    "        self.path_bar_trajectory = path_bar_trajectory\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def visualize(self, **images): \n",
    "        n = len(images)\n",
    "        plt.figure(figsize=(16, 5))\n",
    "        for i, (name, image) in enumerate(images.items()):\n",
    "            plt.subplot(1, n, i + 1)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.title(' '.join(name.split('_')).title())\n",
    "            plt.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_sync_videos(self, path, cap1, cap2, name1 = \"\", name2 = \"\",save_video = False):\n",
    "        width = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        if save_video:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            fps = 30\n",
    "            out = cv2.VideoWriter(\"./\"+name1+\"VS\"+name2+\".mp4\", fourcc, fps, (width*2,height))\n",
    "\n",
    "        for i,j in path:\n",
    "            cap1.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret1, frame1 = cap1.read()\n",
    "            cap2.set(cv2.CAP_PROP_POS_FRAMES, j)\n",
    "            ret2, frame2 = cap2.read()\n",
    "            if not ret1:\n",
    "                frame1 = np.zeros_like(frame2)\n",
    "            if not ret2:\n",
    "                frame2 = np.zeros_like(frame1)\n",
    "            frame2 = cv2.resize(frame2,(width, height)) \n",
    "            frame = cv2.resize(frame1,(width, height)) \n",
    "            frame = np.hstack((frame1, frame2))\n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(frame)\n",
    "            plt.show()\n",
    "            if save_video: out.write(frame)\n",
    "        if save_video: out.release()\n",
    "\n",
    "    def get_frame(self, cap, i):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        res, frame = cap.read()\n",
    "        assert res\n",
    "        img1 = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        return img1\n",
    "\n",
    "    def normalizar_posiciones_y(self, posiciones_y):\n",
    "        pos_min = min(posiciones_y)\n",
    "        pos_max = max(posiciones_y)\n",
    "        amplitud_vertical = pos_max - pos_min\n",
    "        if amplitud_vertical == 0:\n",
    "            factor_escala = 1\n",
    "        else:\n",
    "            factor_escala = 1 / amplitud_vertical\n",
    "        posiciones_y_normalizadas = np.array([[(pos - pos_min) * factor_escala] for pos in posiciones_y])\n",
    "        return posiciones_y_normalizadas\n",
    "\n",
    "    def get_trajectory(self, data, add_negative = False):\n",
    "        vertical_trajectory = np.zeros(len(data))\n",
    "        trayectoria_normalizada = []\n",
    "        negativos = []\n",
    "        for cnt, _ in enumerate(data):\n",
    "            if len(data[cnt][0]) > 0:\n",
    "                x1,y1,x2,y2,_ = data[cnt][0][0]\n",
    "                center_x, center_y = int(x1 + (x2-x1)/2) , int(y1 + (y2 - y1)/2)\n",
    "                vertical_trajectory[cnt] = center_y\n",
    "            else:\n",
    "                indices = np.nonzero(vertical_trajectory)[0]\n",
    "                if len(indices) > 0:\n",
    "                    j = indices[indices < cnt][-1]\n",
    "                    value = vertical_trajectory[j]\n",
    "                    vertical_trajectory[cnt] = value\n",
    "                negativos.append(cnt)\n",
    "\n",
    "        trayectoria_normalizada = self.normalizar_posiciones_y(vertical_trajectory)\n",
    "        if add_negative:\n",
    "            for x in negativos:\n",
    "                trayectoria_normalizada[x] = -1\n",
    "        return trayectoria_normalizada\n",
    "\n",
    "    def k_means_on_trajectory(self, i_frame, trajectory, k=4, max = False):\n",
    "        trajectory = np.array(trajectory)\n",
    "        kmedias = KMeans(n_clusters=k, random_state=0)\n",
    "        kmedias.fit(trajectory)\n",
    "        etiquetas = kmedias.labels_\n",
    "        positive_cluster = etiquetas[i_frame]\n",
    "        indices_cluster_diferente = np.where(etiquetas != positive_cluster)[0]\n",
    "        frame_negativo_index = np.random.choice(indices_cluster_diferente)\n",
    "        if max:\n",
    "            indices_cluster_diferente = np.where(etiquetas != positive_cluster)[0]\n",
    "            positive_trajectory = trajectory[i_frame]\n",
    "            distancias = np.linalg.norm(trajectory[indices_cluster_diferente] - positive_trajectory, axis=1)\n",
    "            frame_negativo_index = indices_cluster_diferente[np.argmax(distancias)]\n",
    "        return frame_negativo_index\n",
    "\n",
    "    def dividir_y_seleccionar_valores(self,arr):\n",
    "        n = len(arr)\n",
    "        segmentos = np.linspace(0, n, num=5, dtype=int)\n",
    "        rangos = [(segmentos[i], segmentos[i+1]) for i in range(4)]\n",
    "        \n",
    "        valores_seleccionados = []\n",
    "        for rango in rangos:\n",
    "            inicio, fin = rango\n",
    "            valores_seleccionados.append(np.random.choice(arr[inicio:fin]))\n",
    "        \n",
    "        return valores_seleccionados\n",
    "\n",
    "    def save_triplet(self, images, name1, name2, cnt):\n",
    "        for key, image in images.items():\n",
    "            save_filename = f\"{name1}+{name2}+{key}_{cnt}.jpg\"\n",
    "            save_filepath = os.path.join(self.save_path+key+\"/\", save_filename)\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image_rgb = cv2.resize(image_rgb, (320, 320))\n",
    "            cv2.imwrite(save_filepath, image_rgb)\n",
    "\n",
    "    def get_triplets(self, plot_triplet = False, plot_video = False):\n",
    "        files = [f for f in os.listdir(self.path_json)]\n",
    "        files = sorted(files)\n",
    "        for j, file in enumerate(files):\n",
    "            name = file.split(\"-\")[0]\n",
    "            #if name != \"12775_5\": continue\n",
    "            \n",
    "            path = os.path.join(self.path_json, file)\n",
    "            with open(path) as f:\n",
    "                data = json.load(f)\n",
    "            best = data\n",
    "\n",
    "            path_trajectory = os.path.join(self.path_bar_trajectory, name+\".json\")\n",
    "            with open(path_trajectory) as f:\n",
    "                data = json.load(f)\n",
    "            trajectory_anchor = self.get_trajectory(data, add_negative = True)\n",
    "            cnt = 0\n",
    "            for video_data in best:\n",
    "                another_name = video_data[0][0]\n",
    "                path = video_data[0][1]\n",
    "                cap1 = cv2.VideoCapture(self.path_videos+name+'.mp4')\n",
    "                cap2 = cv2.VideoCapture(self.path_videos+another_name+'.mp4')  \n",
    "                if plot_video:\n",
    "                    self.plot_sync_videos(path, cap1, cap2)\n",
    "                path_trajectory = os.path.join(self.path_bar_trajectory, another_name+\".json\")\n",
    "                with open(path_trajectory) as f:\n",
    "                    data = json.load(f)\n",
    "                trajectory_positive = self.get_trajectory(data, add_negative = True)\n",
    "                trajectory_ = self.get_trajectory(data)\n",
    "                #print(path)\n",
    "                arreglo = np.arange(len(path)) \n",
    "                valores_seleccionados = self.dividir_y_seleccionar_valores(arreglo)\n",
    "                for value in valores_seleccionados:\n",
    "                    x, y = path[value]\n",
    "                    \n",
    "                    if trajectory_anchor[x] == -1 or trajectory_positive[y] == -1:\n",
    "                        indices_validos_A = [i for i, valor in enumerate(trajectory_anchor) if valor != -1]\n",
    "                        indices_validos_B = [i for i, valor in enumerate(trajectory_positive) if valor != -1]\n",
    "                        if len(indices_validos_A) and len(indices_validos_B):\n",
    "                            for i, (a,b) in enumerate(path):\n",
    "                                if a in indices_validos_A and b in indices_validos_B:\n",
    "                                    x, y = path[i] \n",
    "                                    break\n",
    "                        else:\n",
    "                            continue\n",
    "                    z = self.k_means_on_trajectory(y,trajectory_)\n",
    "                    images = {'Anchor':self.get_frame(cap1,x),'Positive':self.get_frame(cap2,y),'Negative':self.get_frame(cap2,z)}\n",
    "                    self.save_triplet(images, name, another_name, cnt)\n",
    "                    if plot_triplet:\n",
    "                        self.visualize(**images)\n",
    "                    cnt+=1\n",
    "                \n",
    "            print(\"El video {} {} ha sido procesado\".format(name,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El video 11681_3 0 ha sido procesado\n",
      "El video 11882_1 1 ha sido procesado\n",
      "El video 11897_2 2 ha sido procesado\n",
      "El video 11901_1 3 ha sido procesado\n",
      "El video 12422_1 4 ha sido procesado\n",
      "El video 12769_3 5 ha sido procesado\n",
      "El video 12775_5 6 ha sido procesado\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m save_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./ssl_triplets_dataset_ohp/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m g \u001b[39m=\u001b[39m generate_triplets(path, path_videos, path_bar_trajectory, save_path)\n\u001b[0;32m----> 7\u001b[0m g\u001b[39m.\u001b[39;49mget_triplets()\n",
      "Cell \u001b[0;32mIn[39], line 166\u001b[0m, in \u001b[0;36mgenerate_triplets.get_triplets\u001b[0;34m(self, plot_triplet, plot_video)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_means_on_trajectory(y,trajectory_)\n\u001b[1;32m    167\u001b[0m images \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mAnchor\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_frame(cap1,x),\u001b[39m'\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_frame(cap2,y),\u001b[39m'\u001b[39m\u001b[39mNegative\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_frame(cap2,z)}\n\u001b[1;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_triplet(images, name, another_name, cnt)\n",
      "Cell \u001b[0;32mIn[39], line 89\u001b[0m, in \u001b[0;36mgenerate_triplets.k_means_on_trajectory\u001b[0;34m(self, i_frame, trajectory, k, max)\u001b[0m\n\u001b[1;32m     87\u001b[0m trajectory \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(trajectory)\n\u001b[1;32m     88\u001b[0m kmedias \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39mk, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m kmedias\u001b[39m.\u001b[39;49mfit(trajectory)\n\u001b[1;32m     90\u001b[0m etiquetas \u001b[39m=\u001b[39m kmedias\u001b[39m.\u001b[39mlabels_\n\u001b[1;32m     91\u001b[0m positive_cluster \u001b[39m=\u001b[39m etiquetas[i_frame]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1468\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitialization complete\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1467\u001b[0m \u001b[39m# run a k-means once\u001b[39;00m\n\u001b[0;32m-> 1468\u001b[0m labels, inertia, centers, n_iter_ \u001b[39m=\u001b[39m kmeans_single(\n\u001b[1;32m   1469\u001b[0m     X,\n\u001b[1;32m   1470\u001b[0m     sample_weight,\n\u001b[1;32m   1471\u001b[0m     centers_init,\n\u001b[1;32m   1472\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   1473\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1474\u001b[0m     tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tol,\n\u001b[1;32m   1475\u001b[0m     n_threads\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_n_threads,\n\u001b[1;32m   1476\u001b[0m )\n\u001b[1;32m   1478\u001b[0m \u001b[39m# determine if these results are the best so far\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[39m# we chose a new run if it has a better inertia and the clustering is\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \u001b[39m# different from the best so far (it's possible that the inertia is\u001b[39;00m\n\u001b[1;32m   1481\u001b[0m \u001b[39m# slightly better even if the clustering is the same with potentially\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \u001b[39m# permuted labels, due to rounding errors)\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \u001b[39mif\u001b[39;00m best_inertia \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   1484\u001b[0m     inertia \u001b[39m<\u001b[39m best_inertia\n\u001b[1;32m   1485\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_same_clustering(labels, best_labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_clusters)\n\u001b[1;32m   1486\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:731\u001b[0m, in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, sample_weight, centers_init, max_iter, verbose, tol, n_threads)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m strict_convergence:\n\u001b[1;32m    718\u001b[0m         \u001b[39m# rerun E-step so that predicted labels match cluster centers\u001b[39;00m\n\u001b[1;32m    719\u001b[0m         lloyd_iter(\n\u001b[1;32m    720\u001b[0m             X,\n\u001b[1;32m    721\u001b[0m             sample_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    728\u001b[0m             update_centers\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    729\u001b[0m         )\n\u001b[0;32m--> 731\u001b[0m inertia \u001b[39m=\u001b[39m _inertia(X, sample_weight, centers, labels, n_threads)\n\u001b[1;32m    733\u001b[0m \u001b[39mreturn\u001b[39;00m labels, inertia, centers, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"./alinged_videos_OHP_top/\"\n",
    "path_videos = \"./videos/\"\n",
    "path_bar_trajectory = \"./bar_trajectories_raw/\"\n",
    "save_path = \"./ssl_triplets_dataset_ohp/\"\n",
    "\n",
    "g = generate_triplets(path, path_videos, path_bar_trajectory, save_path)\n",
    "g.get_triplets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
