{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import random \n",
    "random_seed = 123  \n",
    "random.seed(random_seed)\n",
    "from sklearn.metrics import f1_score\n",
    "from datasetSSL import VideoDatasetSSL\n",
    "from utils import *\n",
    "from models.pytorch_i3d import InceptionI3d\n",
    "from opts import *\n",
    "#from mmcv.runner import freeze_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "torch.cuda.empty_cache()\n",
    "train_batch_size = 6\n",
    "num_workers = 8\n",
    "save_model = 'ssl_ohp'\n",
    "#save_model = 'ssl_squat'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"./FAQA/OHP/Unlabeled/\"\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(VideoDatasetSSL(input_path, 5970),\n",
    "                                                    batch_size=train_batch_size,\n",
    "                                                    num_workers=num_workers,\n",
    "                                                    shuffle=True,\n",
    "                                                    pin_memory=True,\n",
    "                                                    worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionDisentangling(torch.nn.Module):\n",
    "    def __init__(self, f=1024):\n",
    "        super().__init__()\n",
    "        self.backbone = InceptionI3d()\n",
    "        self.backbone.load_state_dict(torch.load(i3d_pretrained_path))\n",
    "        \n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(f, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(512, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = features.squeeze() #[B,1024,1]=>[B,1024]\n",
    "        x = self.head(features)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DistanceRatioLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DistanceRatioLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        dist_pos_sq = torch.sum(torch.pow(anchor - positive, 2), dim=1)\n",
    "        dist_neg_sq = torch.sum(torch.pow(anchor - negative, 2), dim=1)\n",
    "        \n",
    "        dist_pos = torch.exp(-torch.sqrt(dist_pos_sq))\n",
    "        dist_neg = torch.exp(-torch.sqrt(dist_neg_sq))\n",
    "        loss = -torch.log( dist_pos / (dist_pos + dist_neg))\n",
    "        \n",
    "        loss = torch.mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ssl(model, num_epochs, optimizer, criterion, dataloaders):\n",
    "    model.to(device)\n",
    "    best = 100\n",
    "    for epoch in range(num_epochs):        \n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        train_loss = 0.0\n",
    "\n",
    "        with tqdm(total=len(dataloaders['train']), unit=\"batch\", desc=f\"Epoch {epoch}/{num_epochs}\") as tepoch:\n",
    "            for batch_idx, (anchor, positive, negative) in enumerate(dataloaders['train']):\n",
    "                optimizer.zero_grad()\n",
    "                anchor_emb = model(anchor.to(device))\n",
    "                positive_emb = model(positive.to(device))\n",
    "                negative_emb = model(negative.to(device))\n",
    "                loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "                tepoch.update(1)\n",
    "\n",
    "        train_loss /= len(dataloaders['train'])\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}'.format(epoch, num_epochs, train_loss))\n",
    "        if train_loss < best:\n",
    "            best = train_loss\n",
    "            ruta_guardado = '{0}.pt'.format(save_model)\n",
    "            torch.save(model.state_dict(), ruta_guardado)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ssl_model = MotionDisentangling()\n",
    "lr = 1e-4\n",
    "criterion = DistanceRatioLoss()\n",
    "optimizer = torch.optim.Adam(ssl_model.parameters(),lr=lr, weight_decay=1e-5)\n",
    "num_epochs = 20\n",
    "train_ssl(ssl_model, num_epochs,optimizer,criterion, dataloaders)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import VideoDataset\n",
    "from config import get_parser\n",
    "from logger import Logger\n",
    "\n",
    "from utils import *\n",
    "import torch.nn.init as init\n",
    "\n",
    "#model_name = 'squat_kf'\n",
    "#model_name = 'squat_ki'\n",
    "#model_name = 'ohp_e'\n",
    "model_name = 'ohp_k'\n",
    "#data = 'error_knees_inward.json'\n",
    "#data = 'error_knees_forward.json'\n",
    "#model_name = 'ohp_k'\n",
    "#model_name = 'ohp_e'\n",
    "#data = 'error_elbows.json'\n",
    "data = 'error_knees.json'\n",
    "dataset_path = './FAQA/OHP/Labeled/'\n",
    "#dataset_path = './FAQA/Squat/Labeled/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "1582\n",
      "339\n",
      "tensor([0.0010, 0.0018])\n",
      "tensor([1.9242], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "args = Args(dataset_path)\n",
    "s_train = VideoDataset('train', args, data)\n",
    "s_train_loader = torch.utils.data.DataLoader(s_train,\n",
    "                                                       batch_size=4,\n",
    "                                                       num_workers=8,\n",
    "                                                       shuffle=True,\n",
    "                                                       pin_memory=True,\n",
    "                                                       worker_init_fn=worker_init_fn)\n",
    "s_val = VideoDataset('val', args, data)\n",
    "s_val_loader  = torch.utils.data.DataLoader(s_val,\n",
    "                                                      batch_size=4,\n",
    "                                                      num_workers=8,\n",
    "                                                      shuffle=False,\n",
    "                                                      pin_memory=True,\n",
    "                                                      worker_init_fn=worker_init_fn)\n",
    "print(s_train.__getitem__(0)['video'].shape)\n",
    "print(s_train.__len__())\n",
    "print(s_val.__len__())\n",
    "\n",
    "labels = np.array(s_train.getlabels()) #sin encabezado\n",
    "class_frequencies = torch.bincount(torch.IntTensor(labels))\n",
    "class_weights = 1.0 / class_frequencies\n",
    "print(class_weights)\n",
    "\n",
    "\n",
    "num_positive = np.sum(labels == 1)\n",
    "num_negative = np.sum(labels == 0)\n",
    "weight_positive = num_negative / (num_positive + num_negative)\n",
    "weight_negative = num_positive / (num_positive + num_negative)\n",
    "#print(weight_positive, weight_negative)\n",
    "weights = torch.FloatTensor ([ num_negative / num_positive]).to(device)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W_BCEWithLogitsLoss(torch.nn.Module): \n",
    "    \n",
    "    def __init__(self, w_p = None, w_n = None):\n",
    "        super(W_BCEWithLogitsLoss, self).__init__()\n",
    "        \n",
    "        self.w_p = w_p\n",
    "        self.w_n = w_n\n",
    "        \n",
    "    def forward(self, ps, labels, epsilon = 1e-7):\n",
    "        \n",
    "        loss_pos = -1 * torch.mean(self.w_p * labels * torch.log(ps + epsilon))\n",
    "        loss_neg = -1 * torch.mean(self.w_n * (1-labels) * torch.log((1-ps) + epsilon))\n",
    "        \n",
    "        loss = loss_pos + loss_neg\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTModel(torch.nn.Module):\n",
    "    def __init__(self, n_outputs=1):\n",
    "        super().__init__()\n",
    "        self.backbone = InceptionI3d()\n",
    "        \n",
    "        state_dict = torch.load('./models/{0}.pt'.format(save_model))\n",
    "        \n",
    "        mapped_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            if k.startswith('backbone.'):\n",
    "                k = k[len('backbone.'):]  # Remove the 'backbone.' prefix\n",
    "                mapped_state_dict[k] = v\n",
    "\n",
    "        self.backbone.load_state_dict(mapped_state_dict)\n",
    "\n",
    "        for i, param in enumerate(self.backbone.parameters()):\n",
    "            param.requires_grad = False \n",
    "       \n",
    "        #for name, param in self.backbone.named_parameters():\n",
    "        #    if 'Conv3d_1a_7x7' in name or 'Conv3d_2b_1x1' in name or 'Conv3d_2c_3x3' in name or 'Mixed_3b' in name:\n",
    "        #        param.requires_grad = False\n",
    "\n",
    "        #path = \"./models/rgb_i3d_pretrained.pt\"\n",
    "        #self.backbone.load_state_dict(torch.load(path))\n",
    "            \n",
    "        feature_dim = 1024 #1024\n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feature_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(128, n_outputs)\n",
    "        )\n",
    "\n",
    "        self.getprob = nn.Sigmoid() \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = torch.mean(x,-1)\n",
    "        x = self.head(x)\n",
    "        #x = self.getprob(x) #BCEwithlogits already has\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_FTModel(model, num_epochs, optimizer, criterion, train_loader, val_loader, scheduler, model_name):\n",
    "    hist = {'loss': [], 'acc': [], 'test_acc': []}\n",
    "    best = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        #true_scores, pred_scores, keys_list = [], [], []\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data in tepoch:\n",
    "                videos = data['video'].to(device)\n",
    "                videos.transpose_(1, 2)\n",
    "                batch_size, C, frames, H, W = videos.shape\n",
    "                labels = torch.tensor(data['final_score'].numpy().reshape((batch_size, -1))).to(device).float()\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(videos)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            with tqdm(val_loader, unit=\"batch\") as tepoch:\n",
    "                for data in tepoch:\n",
    "                    videos = data['video'].to(device)\n",
    "                    videos.transpose_(1, 2)\n",
    "                    batch_size, C, frames, H, W = videos.shape\n",
    "                    tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "                    #labels = torch.tensor(data['final_score'].numpy()).to(device)\n",
    "                    labels = torch.tensor(data['final_score'].numpy().reshape((batch_size, -1))).to(device).float()\n",
    "                    outputs = model(videos) #Logits\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    tepoch.set_postfix(loss=loss.item())\n",
    "                    pred_cls = []\n",
    "                    m = nn.Sigmoid() \n",
    "                    outputs = m(outputs)\n",
    "                    for i in range(len(outputs)):\n",
    "                        pred_cls.append(1 if outputs[i] > 0.5 else 0)\n",
    "                    y_true.extend(data['final_score'].numpy().reshape((batch_size, -1)).flatten().tolist())\n",
    "                    y_pred.extend(pred_cls)\n",
    "                \n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        #scheduler.step(val_loss)\n",
    "        #scheduler.step()\n",
    "        print('Epoch [{}/{}], Train Loss: {:.7f} ,Val Loss: {:.7f}'.format(epoch+1, num_epochs, train_loss, val_loss))\n",
    "        f1 = f1_score(y_true, y_pred,average='macro')\n",
    "        f1score_class_1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "        f1score_class_0 = f1_score(y_true, y_pred, pos_label=0)\n",
    "        print('F1 score on the val: {:.7f}, F1 Class 1: {:.7f}, F1 Class 0: {:.7f}'.format(f1, f1score_class_1, f1score_class_0))\n",
    "\n",
    "        if f1score_class_1 > best:\n",
    "            best = f1score_class_1\n",
    "            print('-----New best found!-----')\n",
    "            checkpoint = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "            }\n",
    "\n",
    "            torch.save(checkpoint, 'checkpoint_supervied_{0}_bestf1.pt'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSLeval():\n",
    "    ft_model = FTModel()\n",
    "    \n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            init.kaiming_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    ft_model.apply(init_weights)\n",
    "    ft_model.to(device)\n",
    "    optimizer = torch.optim.Adam(ft_model.parameters(), lr=1e-4, weight_decay=1e-5)    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
    "    #criterion = nn.BCELoss()\n",
    "    lambda1 = lambda epoch: 0.95\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=2, verbose = True)\n",
    "    #scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lambda1, verbose = True)\n",
    "    scheduler = None\n",
    "    num_epochs = 20\n",
    "    train_FTModel(ft_model, num_epochs, optimizer, criterion,s_train_loader, s_val_loader, scheduler, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 396/396 [01:06<00:00,  5.99batch/s, loss=0.747]\n",
      "Epoch 1/20: 100%|██████████| 85/85 [00:14<00:00,  5.75batch/s, loss=0.763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.8951740 ,Val Loss: 0.8129284\n",
      "F1 score on the val: 0.6880129, F1 Class 1: 0.5932203, F1 Class 0: 0.7828054\n",
      "-----New best found!-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 396/396 [01:05<00:00,  6.04batch/s, loss=0.877]\n",
      "Epoch 2/20: 100%|██████████| 85/85 [00:14<00:00,  5.75batch/s, loss=0.756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Train Loss: 0.8569574 ,Val Loss: 0.7826767\n",
      "F1 score on the val: 0.6999882, F1 Class 1: 0.5982143, F1 Class 0: 0.8017621\n",
      "-----New best found!-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 396/396 [01:04<00:00,  6.11batch/s, loss=0.595]\n",
      "Epoch 3/20: 100%|██████████| 85/85 [00:14<00:00,  5.93batch/s, loss=0.773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Train Loss: 0.8537933 ,Val Loss: 0.7747755\n",
      "F1 score on the val: 0.6946903, F1 Class 1: 0.5929204, F1 Class 0: 0.7964602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 396/396 [01:03<00:00,  6.23batch/s, loss=0.382]\n",
      "Epoch 4/20: 100%|██████████| 85/85 [00:14<00:00,  6.04batch/s, loss=0.73] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Train Loss: 0.8382283 ,Val Loss: 0.7677028\n",
      "F1 score on the val: 0.6737122, F1 Class 1: 0.5726496, F1 Class 0: 0.7747748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 396/396 [01:03<00:00,  6.22batch/s, loss=0.803]\n",
      "Epoch 5/20: 100%|██████████| 85/85 [00:14<00:00,  5.82batch/s, loss=0.811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Train Loss: 0.8362595 ,Val Loss: 0.7711598\n",
      "F1 score on the val: 0.6646119, F1 Class 1: 0.5666667, F1 Class 0: 0.7625571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 396/396 [01:04<00:00,  6.13batch/s, loss=0.631]\n",
      "Epoch 6/20: 100%|██████████| 85/85 [00:14<00:00,  5.85batch/s, loss=0.781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Train Loss: 0.8294275 ,Val Loss: 0.7801286\n",
      "F1 score on the val: 0.6463669, F1 Class 1: 0.5827338, F1 Class 0: 0.7100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 396/396 [01:04<00:00,  6.10batch/s, loss=0.701]\n",
      "Epoch 7/20: 100%|██████████| 85/85 [00:14<00:00,  5.91batch/s, loss=0.759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Train Loss: 0.8234858 ,Val Loss: 0.7587067\n",
      "F1 score on the val: 0.6796192, F1 Class 1: 0.5662100, F1 Class 0: 0.7930283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 396/396 [01:04<00:00,  6.11batch/s, loss=0.42] \n",
      "Epoch 8/20: 100%|██████████| 85/85 [00:14<00:00,  5.89batch/s, loss=0.739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Train Loss: 0.8196698 ,Val Loss: 0.7610041\n",
      "F1 score on the val: 0.6667218, F1 Class 1: 0.5900383, F1 Class 0: 0.7434053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 396/396 [01:05<00:00,  6.08batch/s, loss=0.504]\n",
      "Epoch 9/20: 100%|██████████| 85/85 [00:14<00:00,  5.79batch/s, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Train Loss: 0.8242504 ,Val Loss: 0.7464487\n",
      "F1 score on the val: 0.6889908, F1 Class 1: 0.5779817, F1 Class 0: 0.8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 396/396 [01:05<00:00,  6.02batch/s, loss=0.725]\n",
      "Epoch 10/20: 100%|██████████| 85/85 [00:14<00:00,  5.78batch/s, loss=0.763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Train Loss: 0.8227692 ,Val Loss: 0.7482882\n",
      "F1 score on the val: 0.6814159, F1 Class 1: 0.5752212, F1 Class 0: 0.7876106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:   5%|▌         | 21/396 [00:05<01:31,  4.10batch/s, loss=0.995]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m SSLeval()\n",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36mSSLeval\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m scheduler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[0;32m---> 20\u001b[0m train_FTModel(ft_model, num_epochs, optimizer, criterion,s_train_loader, s_val_loader, scheduler, model_name)\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mtrain_FTModel\u001b[0;34m(model, num_epochs, optimizer, criterion, train_loader, val_loader, scheduler, model_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m batch_size, C, frames, H, W \u001b[39m=\u001b[39m videos\u001b[39m.\u001b[39mshape\n\u001b[1;32m     13\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m'\u001b[39m\u001b[39mfinal_score\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape((batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> 14\u001b[0m tepoch\u001b[39m.\u001b[39;49mset_description(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEpoch \u001b[39;49m\u001b[39m{\u001b[39;49;00mepoch\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mnum_epochs\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m outputs \u001b[39m=\u001b[39m model(videos)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py:1391\u001b[0m, in \u001b[0;36mtqdm.set_description\u001b[0;34m(self, desc, refresh)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdesc \u001b[39m=\u001b[39m desc \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m desc \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m refresh:\n\u001b[0;32m-> 1391\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrefresh()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py:1344\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39macquire()\n\u001b[0;32m-> 1344\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisplay()\n\u001b[1;32m   1345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nolock:\n\u001b[1;32m   1346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py:1492\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[1;32m   1491\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1492\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__str__\u001b[39;49m() \u001b[39mif\u001b[39;49;00m msg \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m msg)\n\u001b[1;32m   1493\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[1;32m   1494\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(\u001b[39m-\u001b[39mpos)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py:347\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_status\u001b[39m(s):\n\u001b[1;32m    346\u001b[0m     len_s \u001b[39m=\u001b[39m disp_len(s)\n\u001b[0;32m--> 347\u001b[0m     fp_write(\u001b[39m'\u001b[39;49m\u001b[39m\\r\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m s \u001b[39m+\u001b[39;49m (\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mmax\u001b[39;49m(last_len[\u001b[39m0\u001b[39;49m] \u001b[39m-\u001b[39;49m len_s, \u001b[39m0\u001b[39;49m)))\n\u001b[1;32m    348\u001b[0m     last_len[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m len_s\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py:341\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfp_write\u001b[39m(s):\n\u001b[1;32m    340\u001b[0m     fp\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(s))\n\u001b[0;32m--> 341\u001b[0m     fp_flush()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/utils.py:127\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    129\u001b[0m         \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merrno \u001b[39m!=\u001b[39m \u001b[39m5\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/ipykernel/iostream.py:526\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mschedule(evt\u001b[39m.\u001b[39mset)\n\u001b[1;32m    525\u001b[0m     \u001b[39m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt\u001b[39m.\u001b[39;49mwait(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_timeout):\n\u001b[1;32m    527\u001b[0m         \u001b[39m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    528\u001b[0m         \u001b[39m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIOStream.flush timed out\u001b[39m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39m__stderr__)\n\u001b[1;32m    530\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SSLeval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_test = VideoDataset('test', args, data)\n",
    "s_test_loader  = torch.utils.data.DataLoader(s_test,\n",
    "                                                      batch_size=4,\n",
    "                                                      num_workers=8,\n",
    "                                                      shuffle=False,\n",
    "                                                      pin_memory=True,\n",
    "                                                      worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_FTModel(model, test_loader):  \n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "            for data in tepoch:\n",
    "                videos = data['video'].to(device)\n",
    "                videos.transpose_(1, 2)\n",
    "                batch_size, C, frames, H, W = videos.shape    \n",
    "                outputs = model(videos)\n",
    "                m = nn.Sigmoid() \n",
    "                outputs = m(outputs)\n",
    "                pred_cls = []\n",
    "                for i in range(len(outputs)):\n",
    "                    pred_cls.append(1 if outputs[i] > 0.5 else 0)\n",
    "                y_true.extend(data['final_score'].numpy().reshape((batch_size, -1)).flatten().tolist())\n",
    "                y_pred.extend(pred_cls)\n",
    "        \n",
    "        f1 = f1_score(y_true, y_pred,average='macro')\n",
    "        f1score_class_1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "        f1score_class_0 = f1_score(y_true, y_pred, pos_label=0)\n",
    "        print('F1 score on the test: {:.7f} C0: {:.2f} C1: {:.2f}'.format(f1,f1score_class_0,f1score_class_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:14<00:00,  5.88batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on the test: 0.6473648 C0: 0.72 C1: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ft_model = FTModel()\n",
    "path = \"./checkpoint_supervied_{0}_bestf1.pt\".format(model_name)\n",
    "checkpoint = torch.load(path)\n",
    "ft_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "ft_model.to(device)\n",
    "eval_FTModel(ft_model, s_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
